{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PRODUCTION-GRADE Speech Analytics Platform with 2025 SOTA Techniques\n",
        "## Incorporating latest research: emotion2vec+, Vision Transformers, Graph Neural Networks\n",
        "### Author: Peter Chika Ozo-ogueji (Data Scientist)"
      ],
      "metadata": {
        "id": "tmLHwPWV7h0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cpV_Ewd7gYO",
        "outputId": "48d9b66d-2c00-4356-a1fb-48b6a27c2932"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ CLEAN AUDIO-ONLY SOTA SPEECH ANALYTICS\n",
            "ğŸ¯ NO SYNTHETIC FEATURES - RELIABLE DEPLOYMENT\n",
            "ğŸ‘¨â€ğŸ’» Author: Peter Chika Ozo-ogueji (Data Scientist)\n",
            "ğŸ”§ FIXED: Removes problematic synthetic features\n",
            "================================================================================\n",
            "âœ… All packages imported successfully!\n",
            "ğŸš€ STARTING CLEAN AUDIO-ONLY SYSTEM...\n",
            "ğŸš« NO synthetic features - reliable deployment\n",
            "ğŸ‘¨â€ğŸ’» Author: Peter Chika Ozo-ogueji (Data Scientist)\n",
            "ğŸ—ï¸ BUILDING CLEAN PRODUCTION SYSTEM\n",
            "ğŸš« NO synthetic features - reliable deployment\n",
            "ğŸ‘¨â€ğŸ’» Author: Peter Chika Ozo-ogueji (Data Scientist)\n",
            "================================================================================\n",
            "ğŸ”‘ Setting up Kaggle API...\n",
            "âœ… Kaggle API configured!\n",
            "\n",
            "ğŸ“¥ DOWNLOADING PROVEN AUDIO EMOTION DATASETS\n",
            "ğŸ¯ Focus: Real audio signal processing only\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Downloading RAVDESS Audio-Only...\n",
            "   ğŸ¯ Expected Performance: 85%+ with clean features\n",
            "   âœ… Total audio files: 2880\n",
            "\n",
            "ğŸ“Š Downloading CREMA-D Audio...\n",
            "   ğŸ¯ Expected Performance: 80%+ validated\n",
            "   âœ… Total audio files: 12162\n",
            "\n",
            "ğŸ“Š Downloading TESS Clean Audio...\n",
            "   ğŸ¯ Expected Performance: High performance\n",
            "   âœ… Total audio files: 5600\n",
            "\n",
            "ğŸ“Š Downloading EMO-DB German...\n",
            "   ğŸ¯ Expected Performance: 90%+ on clean features\n",
            "   âœ… Total audio files: 535\n",
            "\n",
            "ğŸ“Š Downloading SAVEE British...\n",
            "   ğŸ¯ Expected Performance: Proven benchmark\n",
            "   âœ… Total audio files: 960\n",
            "\n",
            "âœ… Downloaded 5 proven datasets with 22,137 files!\n",
            "\n",
            "ğŸ”§ CLEAN DATASET PREPARATION (AUDIO FEATURES ONLY)\n",
            "Target: 1500 samples per emotion\n",
            "ğŸš« NO synthetic features - reliable deployment\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š Processing RAVDESS Audio-Only...\n",
            "   ğŸ¯ Expected Performance: 85%+ with clean features\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing RAVDESS Audio-Only:   0%|          | 1/2880 [00:27<21:46:26, 27.23s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Extracting 191 CLEAN audio features per file\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing RAVDESS Audio-Only: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2880/2880 [19:08<00:00,  2.51it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… Processed: 2880 samples\n",
            "   âŒ Failed: 0 samples\n",
            "   â“ Unknown emotions: 0 samples\n",
            "   ğŸ“Š Success rate: 100.0%\n",
            "   ğŸ“Š Emotions found: {'neutral': 192, 'happy': 384, 'sad': 384, 'calm': 384, 'fearful': 384, 'disgust': 384, 'surprised': 384, 'angry': 384}\n",
            "\n",
            "ğŸ“Š Processing CREMA-D Audio...\n",
            "   ğŸ¯ Expected Performance: 80%+ validated\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing CREMA-D Audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6000/6000 [34:46<00:00,  2.87it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… Processed: 6000 samples\n",
            "   âŒ Failed: 0 samples\n",
            "   â“ Unknown emotions: 0 samples\n",
            "   ğŸ“Š Success rate: 100.0%\n",
            "   ğŸ“Š Emotions found: {'happy': 939, 'angry': 948, 'neutral': 854, 'disgust': 936, 'fearful': 975, 'surprised': 318, 'sad': 932, 'calm': 98}\n",
            "\n",
            "ğŸ“Š Processing TESS Clean Audio...\n",
            "   ğŸ¯ Expected Performance: High performance\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing TESS Clean Audio: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5600/5600 [10:55<00:00,  8.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… Processed: 2102 samples\n",
            "   âŒ Failed: 0 samples\n",
            "   â“ Unknown emotions: 0 samples\n",
            "   ğŸ“Š Success rate: 37.5%\n",
            "   ğŸ“Š Emotions found: {'surprised': 798, 'fearful': 141, 'neutral': 454, 'happy': 177, 'disgust': 180, 'angry': 168, 'sad': 184}\n",
            "\n",
            "ğŸ“Š Processing EMO-DB German...\n",
            "   ğŸ¯ Expected Performance: 90%+ on clean features\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing EMO-DB German: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 535/535 [00:00<00:00, 138892.83it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… Processed: 0 samples\n",
            "   âŒ Failed: 0 samples\n",
            "   â“ Unknown emotions: 0 samples\n",
            "   ğŸ“Š Success rate: 0.0%\n",
            "   ğŸ“Š Emotions found: {}\n",
            "\n",
            "ğŸ“Š Processing SAVEE British...\n",
            "   ğŸ¯ Expected Performance: Proven benchmark\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing SAVEE British: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 960/960 [00:00<00:00, 185358.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… Processed: 0 samples\n",
            "   âŒ Failed: 0 samples\n",
            "   â“ Unknown emotions: 0 samples\n",
            "   ğŸ“Š Success rate: 0.0%\n",
            "   ğŸ“Š Emotions found: {}\n",
            "\n",
            "ğŸ” DEBUG: Clean emotion extractions:\n",
            "   RAVDESS Audio-Only: 03-01-01-01-02-02-14.wav â†’ neutral\n",
            "   RAVDESS Audio-Only: 03-01-03-01-02-02-21.wav â†’ happy\n",
            "   RAVDESS Audio-Only: 03-01-04-02-02-01-05.wav â†’ sad\n",
            "   RAVDESS Audio-Only: 03-01-02-02-02-01-12.wav â†’ calm\n",
            "   RAVDESS Audio-Only: 03-01-04-02-02-01-20.wav â†’ sad\n",
            "   RAVDESS Audio-Only: 03-01-06-01-01-01-10.wav â†’ fearful\n",
            "   RAVDESS Audio-Only: 03-01-07-02-01-02-23.wav â†’ disgust\n",
            "   RAVDESS Audio-Only: 03-01-04-02-01-02-19.wav â†’ sad\n",
            "   RAVDESS Audio-Only: 03-01-02-02-01-02-15.wav â†’ calm\n",
            "   RAVDESS Audio-Only: 03-01-04-01-02-02-23.wav â†’ sad\n",
            "   RAVDESS Audio-Only: 03-01-08-02-02-01-16.wav â†’ surprised\n",
            "   RAVDESS Audio-Only: 03-01-05-01-01-02-16.wav â†’ angry\n",
            "   RAVDESS Audio-Only: 03-01-06-02-01-02-16.wav â†’ fearful\n",
            "   RAVDESS Audio-Only: 03-01-08-02-01-01-23.wav â†’ surprised\n",
            "   RAVDESS Audio-Only: 03-01-02-02-01-01-22.wav â†’ calm\n",
            "   RAVDESS Audio-Only: 03-01-05-02-01-01-06.wav â†’ angry\n",
            "   RAVDESS Audio-Only: 03-01-02-01-01-01-04.wav â†’ calm\n",
            "   RAVDESS Audio-Only: 03-01-01-01-01-01-18.wav â†’ neutral\n",
            "   RAVDESS Audio-Only: 03-01-03-01-02-02-10.wav â†’ happy\n",
            "   RAVDESS Audio-Only: 03-01-02-01-02-01-11.wav â†’ calm\n",
            "   RAVDESS Audio-Only: 03-01-04-02-01-02-08.wav â†’ sad\n",
            "   RAVDESS Audio-Only: 03-01-07-01-01-01-20.wav â†’ disgust\n",
            "   RAVDESS Audio-Only: 03-01-06-01-01-02-14.wav â†’ fearful\n",
            "   RAVDESS Audio-Only: 03-01-05-02-01-02-05.wav â†’ angry\n",
            "   RAVDESS Audio-Only: 03-01-03-01-02-02-16.wav â†’ happy\n",
            "   RAVDESS Audio-Only: 03-01-02-02-01-01-05.wav â†’ calm\n",
            "   RAVDESS Audio-Only: 03-01-04-02-02-02-05.wav â†’ sad\n",
            "   RAVDESS Audio-Only: 03-01-05-02-01-02-12.wav â†’ angry\n",
            "   RAVDESS Audio-Only: 03-01-07-02-01-02-18.wav â†’ disgust\n",
            "   RAVDESS Audio-Only: 03-01-07-01-02-01-22.wav â†’ disgust\n",
            "\n",
            "ğŸ“Š Clean extraction success rate: 100.0%\n",
            "\n",
            "âœ… CLEAN DATASET PREPARATION COMPLETE!\n",
            "ğŸ“Š Total samples: 10982\n",
            "ğŸ“Š Final emotion distribution: {'neutral': 1500, 'happy': 1500, 'sad': 1500, 'calm': 482, 'fearful': 1500, 'disgust': 1500, 'surprised': 1500, 'angry': 1500}\n",
            "ğŸ“Š CLEAN feature matrix shape: (10982, 191)\n",
            "\n",
            "ğŸ§  CLEAN MODEL TRAINING (AUDIO FEATURES ONLY)\n",
            "ğŸš« NO synthetic features - reliable deployment\n",
            "======================================================================\n",
            "ğŸ“Š Classes: [np.str_('angry'), np.str_('calm'), np.str_('disgust'), np.str_('fearful'), np.str_('happy'), np.str_('neutral'), np.str_('sad'), np.str_('surprised')]\n",
            "ğŸ“Š Class distribution: {np.str_('angry'): np.int64(1500), np.str_('calm'): np.int64(482), np.str_('disgust'): np.int64(1500), np.str_('fearful'): np.int64(1500), np.str_('happy'): np.int64(1500), np.str_('neutral'): np.int64(1500), np.str_('sad'): np.int64(1500), np.str_('surprised'): np.int64(1500)}\n",
            "ğŸ“Š Training set: 8785 samples\n",
            "ğŸ“Š Test set: 2197 samples\n",
            "ğŸ“Š CLEAN audio features: 191\n",
            "\n",
            "ğŸ” CLEAN FEATURE SELECTION...\n",
            "ğŸ“Š Selected clean features: 150\n",
            "\n",
            "âš–ï¸ CLEAN CLASS BALANCING...\n",
            "ğŸ“Š Balanced training set: 9600 samples\n",
            "\n",
            "ğŸ”„ CLEAN MODEL EVALUATION:\n",
            "--------------------------------------------------\n",
            "\n",
            "ğŸ§ª Training Clean XGBoost...\n",
            "   ğŸ“Š CV F1-Score: 0.822 Â± 0.008\n",
            "   ğŸ“Š Test Accuracy: 0.812\n",
            "   ğŸ“Š Test F1-Score: 0.822\n",
            "\n",
            "ğŸ§ª Training Clean LightGBM...\n",
            "   ğŸ“Š CV F1-Score: 0.823 Â± 0.005\n",
            "   ğŸ“Š Test Accuracy: 0.817\n",
            "   ğŸ“Š Test F1-Score: 0.829\n",
            "\n",
            "ğŸ§ª Training Clean Random Forest...\n",
            "   ğŸ“Š CV F1-Score: 0.814 Â± 0.005\n",
            "   ğŸ“Š Test Accuracy: 0.802\n",
            "   ğŸ“Š Test F1-Score: 0.811\n",
            "\n",
            "ğŸ§ª Training Clean SVM...\n",
            "   ğŸ“Š CV F1-Score: 0.823 Â± 0.005\n",
            "   ğŸ“Š Test Accuracy: 0.813\n",
            "   ğŸ“Š Test F1-Score: 0.827\n",
            "\n",
            "ğŸ¤ CREATING CLEAN ENSEMBLE...\n",
            "   ğŸ“Š Clean Ensemble Accuracy: 0.820\n",
            "   ğŸ“Š Clean Ensemble F1-Score: 0.831\n",
            "\n",
            "ğŸ† BEST CLEAN MODEL: Clean Ensemble\n",
            "   ğŸ“Š Test Accuracy: 0.820\n",
            "   ğŸ“Š Test F1-Score: 0.831\n",
            "\n",
            "ğŸ“Š DETAILED CLEAN PERFORMANCE ANALYSIS:\n",
            "------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry      0.780     0.873     0.824       300\n",
            "        calm      0.942     1.000     0.970        97\n",
            "     disgust      0.822     0.707     0.760       300\n",
            "     fearful      0.777     0.743     0.760       300\n",
            "       happy      0.788     0.730     0.758       300\n",
            "     neutral      0.794     0.847     0.819       300\n",
            "         sad      0.782     0.800     0.791       300\n",
            "   surprised      0.958     0.983     0.970       300\n",
            "\n",
            "    accuracy                          0.820      2197\n",
            "   macro avg      0.830     0.835     0.831      2197\n",
            "weighted avg      0.820     0.820     0.819      2197\n",
            "\n",
            "\n",
            "ğŸ‰ CLEAN PRODUCTION SYSTEM COMPLETE!\n",
            "ğŸ“Š Total samples: 10,982\n",
            "ğŸ“Š Best clean model: Clean Ensemble\n",
            "ğŸ“Š Test accuracy: 82.0%\n",
            "ğŸ“Š Test F1-score: 83.1%\n",
            "ğŸ“Š Clean audio features: 191\n",
            "ğŸ“Š Extraction success: 100.0%\n",
            "ğŸš« Synthetic features: REMOVED\n",
            "ğŸ¯ âœ… EXCELLENT PERFORMANCE: 80%+ ACCURACY!\n",
            "\n",
            "================================================================================\n",
            "ğŸ‰ CLEAN AUDIO-ONLY SPEECH ANALYTICS COMPLETE!\n",
            "ğŸš« NO SYNTHETIC FEATURES - DEPLOYMENT READY!\n",
            "ğŸ¯ RELIABLE PERFORMANCE WITH REAL AUDIO FEATURES!\n",
            "ğŸ‘¨â€ğŸ’» Author: Peter Chika Ozo-ogueji (Data Scientist)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ’¾ SAVING CLEAN MODELS FOR DEPLOYMENT\n",
            "================================================================================\n",
            "\n",
            "ğŸ’¾ SAVING CLEAN AUDIO-ONLY MODELS\n",
            "ğŸ“‚ Target directory: clean_audio_models\n",
            "ğŸš« NO synthetic features - deployment ready\n",
            "======================================================================\n",
            "âœ… Saved Clean Model: clean_audio_models/sota_xgboost_model.pkl (99.4 MB)\n",
            "âœ… Saved Clean Scaler: clean_audio_models/robust_scalar.pkl (2.3 KB)\n",
            "âœ… Saved Clean Feature Selector: clean_audio_models/feature_selector.pkl (3.5 KB)\n",
            "âœ… Saved Clean Label Encoder: clean_audio_models/label_encoder.pkl (0.6 KB)\n",
            "âœ… Saved Clean Feature Names: clean_audio_models/feature_names.pkl (191 features, 3.3 KB)\n",
            "âœ… Saved Clean Metadata: clean_audio_models/model_metadata.json (0.9 KB)\n",
            "\n",
            "ğŸ‰ ALL CLEAN MODELS SAVED SUCCESSFULLY!\n",
            "ğŸ“‚ Location: /content/clean_audio_models\n",
            "ğŸ“Š Total size: 99.4 MB\n",
            "ğŸ¯ Model accuracy: 0.820\n",
            "ğŸ“ˆ F1-score: 0.831\n",
            "ğŸ”¬ Clean features: 191\n",
            "ğŸ“š Samples: 10,982\n",
            "ğŸš« Synthetic features: REMOVED\n",
            "\n",
            "ğŸ‰ CLEAN MODELS SAVED SUCCESSFULLY!\n",
            "ğŸš« NO synthetic features - deployment ready!\n",
            "ğŸš€ Your clean audio model should work reliably!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "CLEAN AUDIO-ONLY SOTA Speech Analytics - NO SYNTHETIC FEATURES\n",
        "Real audio signal processing features only for reliable deployment\n",
        "Author: Peter Chika Ozo-ogueji (Data Scientist)\n",
        "\n",
        "FIXED: Removes Vision Transformer, Graph, and Quantum features\n",
        "Target: 80%+ accuracy with deployable reliability\n",
        "\"\"\"\n",
        "\n",
        "print(\"ğŸš€ CLEAN AUDIO-ONLY SOTA SPEECH ANALYTICS\")\n",
        "print(\"ğŸ¯ NO SYNTHETIC FEATURES - RELIABLE DEPLOYMENT\")\n",
        "print(\"ğŸ‘¨â€ğŸ’» Author: Peter Chika Ozo-ogueji (Data Scientist)\")\n",
        "print(\"ğŸ”§ FIXED: Removes problematic synthetic features\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Install packages\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    packages = [\n",
        "        'kaggle', 'librosa', 'soundfile', 'scikit-learn', 'plotly',\n",
        "        'matplotlib', 'seaborn', 'pandas', 'numpy', 'tqdm',\n",
        "        'xgboost', 'lightgbm', 'imbalanced-learn',\n",
        "        'scipy', 'audiomentations'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "        except:\n",
        "            print(f\"Warning: Could not install {package}\")\n",
        "\n",
        "install_packages()\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import re\n",
        "import glob\n",
        "from scipy import stats\n",
        "import time\n",
        "import gc\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… All packages imported successfully!\")\n",
        "\n",
        "# =============================================================================\n",
        "# 1. KAGGLE SETUP (Same as before)\n",
        "# =============================================================================\n",
        "\n",
        "def setup_kaggle():\n",
        "    \"\"\"Setup Kaggle API\"\"\"\n",
        "    print(\"ğŸ”‘ Setting up Kaggle API...\")\n",
        "\n",
        "    kaggle_dirs = ['/root/.kaggle', '/root/.config/kaggle', os.path.expanduser('~/.kaggle')]\n",
        "    for directory in kaggle_dirs:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    kaggle_credentials = {\n",
        "        \"username\": \"peterchikaozoogueji\",\n",
        "        \"key\": \"f2b5ae97165cf3eef611db7624db7192\"\n",
        "    }\n",
        "\n",
        "    for directory in kaggle_dirs:\n",
        "        if os.path.exists(directory):\n",
        "            kaggle_file = os.path.join(directory, 'kaggle.json')\n",
        "            with open(kaggle_file, 'w') as f:\n",
        "                json.dump(kaggle_credentials, f)\n",
        "            os.chmod(kaggle_file, 0o600)\n",
        "\n",
        "    print(\"âœ… Kaggle API configured!\")\n",
        "    return True\n",
        "\n",
        "# =============================================================================\n",
        "# 2. DATASET DOWNLOADING (Same as before)\n",
        "# =============================================================================\n",
        "\n",
        "def download_sota_datasets():\n",
        "    \"\"\"Download proven audio emotion datasets\"\"\"\n",
        "    print(\"\\nğŸ“¥ DOWNLOADING PROVEN AUDIO EMOTION DATASETS\")\n",
        "    print(\"ğŸ¯ Focus: Real audio signal processing only\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    base_path = '/content/clean_audio_datasets' if '/content' in os.getcwd() else './clean_audio_datasets'\n",
        "    os.makedirs(base_path, exist_ok=True)\n",
        "    os.makedirs(f'{base_path}/emotion', exist_ok=True)\n",
        "\n",
        "    # Proven audio emotion datasets\n",
        "    datasets = [\n",
        "        {\n",
        "            'name': 'RAVDESS Audio-Only',\n",
        "            'kaggle_id': 'uwrfkaggler/ravdess-emotional-speech-audio',\n",
        "            'path': f'{base_path}/emotion/ravdess/',\n",
        "            'priority': 1,\n",
        "            'accuracy': '85%+ with clean features'\n",
        "        },\n",
        "        {\n",
        "            'name': 'CREMA-D Audio',\n",
        "            'kaggle_id': 'dmitrybabko/speech-emotion-recognition-en',\n",
        "            'path': f'{base_path}/emotion/crema_d/',\n",
        "            'priority': 1,\n",
        "            'accuracy': '80%+ validated'\n",
        "        },\n",
        "        {\n",
        "            'name': 'TESS Clean Audio',\n",
        "            'kaggle_id': 'ejlok1/toronto-emotional-speech-set-tess',\n",
        "            'path': f'{base_path}/emotion/tess/',\n",
        "            'priority': 1,\n",
        "            'accuracy': 'High performance'\n",
        "        },\n",
        "        {\n",
        "            'name': 'EMO-DB German',\n",
        "            'kaggle_id': 'piyushagni5/berlin-database-of-emotional-speech-emodb',\n",
        "            'path': f'{base_path}/emotion/emodb/',\n",
        "            'priority': 2,\n",
        "            'accuracy': '90%+ on clean features'\n",
        "        },\n",
        "        {\n",
        "            'name': 'SAVEE British',\n",
        "            'kaggle_id': 'barelydedicated/savee-database',\n",
        "            'path': f'{base_path}/emotion/savee/',\n",
        "            'priority': 2,\n",
        "            'accuracy': 'Proven benchmark'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    successful_downloads = []\n",
        "\n",
        "    for dataset in datasets:\n",
        "        try:\n",
        "            print(f\"\\nğŸ“Š Downloading {dataset['name']}...\")\n",
        "            print(f\"   ğŸ¯ Expected Performance: {dataset['accuracy']}\")\n",
        "            os.makedirs(dataset['path'], exist_ok=True)\n",
        "\n",
        "            result = os.system(f\"kaggle datasets download -d {dataset['kaggle_id']} -p {dataset['path']} --unzip\")\n",
        "\n",
        "            if result == 0 or os.listdir(dataset['path']):\n",
        "                audio_files = []\n",
        "                for root, dirs, files in os.walk(dataset['path']):\n",
        "                    for file in files:\n",
        "                        if file.lower().endswith(('.wav', '.mp3', '.flac', '.m4a', '.aac')):\n",
        "                            audio_files.append(os.path.join(root, file))\n",
        "\n",
        "                print(f\"   âœ… Total audio files: {len(audio_files)}\")\n",
        "\n",
        "                successful_downloads.append({\n",
        "                    'name': dataset['name'],\n",
        "                    'path': dataset['path'],\n",
        "                    'file_count': len(audio_files),\n",
        "                    'files': audio_files,\n",
        "                    'priority': dataset['priority'],\n",
        "                    'accuracy': dataset['accuracy']\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Error downloading {dataset['name']}: {e}\")\n",
        "\n",
        "    total_files = sum(d['file_count'] for d in successful_downloads)\n",
        "    print(f\"\\nâœ… Downloaded {len(successful_downloads)} proven datasets with {total_files:,} files!\")\n",
        "\n",
        "    return successful_downloads\n",
        "\n",
        "# =============================================================================\n",
        "# 3. CLEAN AUDIO-ONLY FEATURE EXTRACTION\n",
        "# =============================================================================\n",
        "\n",
        "class CleanAudioFeatureExtractor:\n",
        "    \"\"\"Clean audio feature extraction - NO SYNTHETIC FEATURES\"\"\"\n",
        "\n",
        "    def __init__(self, sample_rate=22050):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.feature_names = None\n",
        "\n",
        "    def extract_clean_audio_features(self, audio_file_path):\n",
        "        \"\"\"Extract ONLY real audio signal processing features\"\"\"\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio, sr = librosa.load(audio_file_path, sr=self.sample_rate, duration=3.0)\n",
        "            if audio is None or len(audio) == 0:\n",
        "                return {}\n",
        "\n",
        "            # Clean audio\n",
        "            if not np.isfinite(audio).all():\n",
        "                audio = np.nan_to_num(audio, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            if np.max(np.abs(audio)) > 0:\n",
        "                audio = librosa.util.normalize(audio)\n",
        "\n",
        "            features = {}\n",
        "\n",
        "            # 1. COMPREHENSIVE MFCC FEATURES (104 features)\n",
        "            features.update(self._extract_mfcc_features(audio, sr))\n",
        "\n",
        "            # 2. SPECTRAL FEATURES (16 features)\n",
        "            features.update(self._extract_spectral_features(audio, sr))\n",
        "\n",
        "            # 3. CHROMA FEATURES (24 features)\n",
        "            features.update(self._extract_chroma_features(audio, sr))\n",
        "\n",
        "            # 4. PROSODIC FEATURES (11 features)\n",
        "            features.update(self._extract_prosodic_features(audio, sr))\n",
        "\n",
        "            # 5. ADVANCED SPECTRAL FEATURES (16 features)\n",
        "            features.update(self._extract_advanced_spectral_features(audio, sr))\n",
        "\n",
        "            # 6. HARMONIC FEATURES (15 features)\n",
        "            features.update(self._extract_harmonic_features(audio, sr))\n",
        "\n",
        "            # 7. TEMPORAL FEATURES (5 features)\n",
        "            features.update(self._extract_temporal_features(audio, sr))\n",
        "\n",
        "            # Clean all features\n",
        "            for key, value in features.items():\n",
        "                if np.isnan(value) or np.isinf(value):\n",
        "                    features[key] = 0.0\n",
        "\n",
        "            if self.feature_names is None:\n",
        "                self.feature_names = list(features.keys())\n",
        "                print(f\"âœ… Extracting {len(self.feature_names)} CLEAN audio features per file\")\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.feature_names is not None:\n",
        "                return {name: 0.0 for name in self.feature_names}\n",
        "            return {}\n",
        "\n",
        "    def _extract_mfcc_features(self, audio, sr):\n",
        "        \"\"\"Comprehensive MFCC features - most important for emotion recognition\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Enhanced MFCC extraction\n",
        "            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, n_fft=2048, hop_length=512)\n",
        "            mfcc_delta = librosa.feature.delta(mfccs)\n",
        "            mfcc_delta2 = librosa.feature.delta(mfccs, order=2)\n",
        "\n",
        "            for i in range(13):\n",
        "                # Comprehensive statistics for each MFCC coefficient\n",
        "                features[f'mfcc_{i}_mean'] = float(np.mean(mfccs[i]))\n",
        "                features[f'mfcc_{i}_std'] = float(np.std(mfccs[i]))\n",
        "                features[f'mfcc_{i}_max'] = float(np.max(mfccs[i]))\n",
        "                features[f'mfcc_{i}_min'] = float(np.min(mfccs[i]))\n",
        "                features[f'mfcc_{i}_skew'] = float(stats.skew(mfccs[i]))\n",
        "                features[f'mfcc_{i}_kurtosis'] = float(stats.kurtosis(mfccs[i]))\n",
        "                features[f'mfcc_delta_{i}_mean'] = float(np.mean(mfcc_delta[i]))\n",
        "                features[f'mfcc_delta2_{i}_mean'] = float(np.mean(mfcc_delta2[i]))\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback MFCC features\n",
        "            for i in range(13):\n",
        "                for stat in ['mean', 'std', 'max', 'min', 'skew', 'kurtosis']:\n",
        "                    features[f'mfcc_{i}_{stat}'] = 0.0\n",
        "                features[f'mfcc_delta_{i}_mean'] = 0.0\n",
        "                features[f'mfcc_delta2_{i}_mean'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_spectral_features(self, audio, sr):\n",
        "        \"\"\"Basic spectral features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]\n",
        "            zero_crossing_rate = librosa.feature.zero_crossing_rate(audio)[0]\n",
        "\n",
        "            for name, feature_array in [\n",
        "                ('spectral_centroid', spectral_centroids),\n",
        "                ('spectral_rolloff', spectral_rolloff),\n",
        "                ('spectral_bandwidth', spectral_bandwidth),\n",
        "                ('zero_crossing_rate', zero_crossing_rate)\n",
        "            ]:\n",
        "                features[f'{name}_mean'] = float(np.mean(feature_array))\n",
        "                features[f'{name}_std'] = float(np.std(feature_array))\n",
        "                features[f'{name}_max'] = float(np.max(feature_array))\n",
        "                features[f'{name}_skew'] = float(stats.skew(feature_array))\n",
        "\n",
        "        except Exception as e:\n",
        "            for name in ['spectral_centroid', 'spectral_rolloff', 'spectral_bandwidth', 'zero_crossing_rate']:\n",
        "                for stat in ['mean', 'std', 'max', 'skew']:\n",
        "                    features[f'{name}_{stat}'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_chroma_features(self, audio, sr):\n",
        "        \"\"\"Chroma features for harmonic content\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            chroma = librosa.feature.chroma_stft(y=audio, sr=sr, n_chroma=12)\n",
        "            for i in range(12):\n",
        "                features[f'chroma_{i}_mean'] = float(np.mean(chroma[i]))\n",
        "                features[f'chroma_{i}_std'] = float(np.std(chroma[i]))\n",
        "\n",
        "        except Exception as e:\n",
        "            for i in range(12):\n",
        "                features[f'chroma_{i}_mean'] = 0.0\n",
        "                features[f'chroma_{i}_std'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_prosodic_features(self, audio, sr):\n",
        "        \"\"\"Prosodic features (F0, energy, etc.)\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # F0 extraction (fixed - no threshold parameter)\n",
        "            f0 = librosa.yin(audio, fmin=50, fmax=400)\n",
        "            f0_clean = f0[f0 > 0]\n",
        "\n",
        "            if len(f0_clean) > 0:\n",
        "                features['f0_mean'] = float(np.mean(f0_clean))\n",
        "                features['f0_std'] = float(np.std(f0_clean))\n",
        "                features['f0_range'] = float(np.max(f0_clean) - np.min(f0_clean))\n",
        "                features['f0_jitter'] = float(np.mean(np.abs(np.diff(f0_clean))) / np.mean(f0_clean)) if len(f0_clean) > 1 else 0.0\n",
        "                features['f0_shimmer'] = float(np.std(f0_clean) / np.mean(f0_clean)) if np.mean(f0_clean) > 0 else 0.0\n",
        "                features['f0_slope'] = float(np.polyfit(range(len(f0_clean)), f0_clean, 1)[0]) if len(f0_clean) > 1 else 0.0\n",
        "                features['f0_curvature'] = float(np.polyfit(range(len(f0_clean)), f0_clean, 2)[0]) if len(f0_clean) > 2 else 0.0\n",
        "            else:\n",
        "                for feat in ['f0_mean', 'f0_std', 'f0_range', 'f0_jitter', 'f0_shimmer', 'f0_slope', 'f0_curvature']:\n",
        "                    features[feat] = 0.0\n",
        "\n",
        "            # Energy features\n",
        "            rms = librosa.feature.rms(y=audio)[0]\n",
        "            features['energy_mean'] = float(np.mean(rms))\n",
        "            features['energy_std'] = float(np.std(rms))\n",
        "            features['energy_skew'] = float(stats.skew(rms))\n",
        "            features['energy_kurtosis'] = float(stats.kurtosis(rms))\n",
        "\n",
        "        except Exception as e:\n",
        "            for feat in ['f0_mean', 'f0_std', 'f0_range', 'f0_jitter', 'f0_shimmer',\n",
        "                        'f0_slope', 'f0_curvature', 'energy_mean', 'energy_std', 'energy_skew', 'energy_kurtosis']:\n",
        "                features[feat] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_advanced_spectral_features(self, audio, sr):\n",
        "        \"\"\"Advanced spectral features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Spectral contrast (7 bands Ã— 2 stats = 14 features)\n",
        "            spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
        "            for i in range(min(7, spectral_contrast.shape[0])):\n",
        "                features[f'spectral_contrast_{i}_mean'] = float(np.mean(spectral_contrast[i]))\n",
        "                features[f'spectral_contrast_{i}_std'] = float(np.std(spectral_contrast[i]))\n",
        "\n",
        "            # Spectral flatness (2 features)\n",
        "            spectral_flatness = librosa.feature.spectral_flatness(y=audio)[0]\n",
        "            features['spectral_flatness_mean'] = float(np.mean(spectral_flatness))\n",
        "            features['spectral_flatness_std'] = float(np.std(spectral_flatness))\n",
        "\n",
        "        except Exception as e:\n",
        "            for i in range(7):\n",
        "                features[f'spectral_contrast_{i}_mean'] = 0.0\n",
        "                features[f'spectral_contrast_{i}_std'] = 0.0\n",
        "            features['spectral_flatness_mean'] = 0.0\n",
        "            features['spectral_flatness_std'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_harmonic_features(self, audio, sr):\n",
        "        \"\"\"Harmonic and tonal features\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Tonnetz (6 Ã— 2 = 12 features)\n",
        "            tonnetz = librosa.feature.tonnetz(y=audio, sr=sr)\n",
        "            for i in range(min(6, tonnetz.shape[0])):\n",
        "                features[f'tonnetz_{i}_mean'] = float(np.mean(tonnetz[i]))\n",
        "                features[f'tonnetz_{i}_std'] = float(np.std(tonnetz[i]))\n",
        "\n",
        "            # Harmonic-percussive separation (3 features)\n",
        "            y_harmonic, y_percussive = librosa.effects.hpss(audio)\n",
        "            features['harmonic_energy'] = float(np.mean(y_harmonic**2))\n",
        "            features['percussive_energy'] = float(np.mean(y_percussive**2))\n",
        "            features['harmonic_percussive_ratio'] = float(features['harmonic_energy'] / (features['percussive_energy'] + 1e-8))\n",
        "\n",
        "        except Exception as e:\n",
        "            for i in range(6):\n",
        "                features[f'tonnetz_{i}_mean'] = 0.0\n",
        "                features[f'tonnetz_{i}_std'] = 0.0\n",
        "            features['harmonic_energy'] = 0.0\n",
        "            features['percussive_energy'] = 0.0\n",
        "            features['harmonic_percussive_ratio'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_temporal_features(self, audio, sr):\n",
        "        \"\"\"Temporal features (rhythm, beats, etc.)\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Tempo and beat features\n",
        "            tempo, beats = librosa.beat.beat_track(y=audio, sr=sr)\n",
        "            features['tempo'] = float(tempo)\n",
        "            features['beat_count'] = float(len(beats))\n",
        "            features['beat_variance'] = float(np.var(np.diff(beats))) if len(beats) > 1 else 0.0\n",
        "\n",
        "            # Onset detection\n",
        "            onset_frames = librosa.onset.onset_detect(y=audio, sr=sr)\n",
        "            features['onset_count'] = float(len(onset_frames))\n",
        "            features['onset_rate'] = float(len(onset_frames) / (len(audio) / sr))\n",
        "\n",
        "        except Exception as e:\n",
        "            for feat in ['tempo', 'beat_count', 'beat_variance', 'onset_count', 'onset_rate']:\n",
        "                features[feat] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "# =============================================================================\n",
        "# 4. EMOTION EXTRACTION (Same as before)\n",
        "# =============================================================================\n",
        "\n",
        "class CleanEmotionExtractor:\n",
        "    \"\"\"Clean emotion extraction - same as before\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.debug_extractions = []\n",
        "        self.extraction_stats = {'total': 0, 'success': 0, 'failed': 0}\n",
        "\n",
        "        self.emotion_patterns = {\n",
        "            'angry': ['angry', 'anger', 'ang', 'mad', 'furious', 'rage'],\n",
        "            'calm': ['calm', 'neutral-calm', 'peaceful', 'composed'],\n",
        "            'disgust': ['disgust', 'disgusted', 'dis', 'revulsion', 'contempt'],\n",
        "            'fearful': ['fear', 'fearful', 'fea', 'afraid', 'scared', 'anxiety'],\n",
        "            'happy': ['happy', 'happiness', 'hap', 'joy', 'joyful', 'pleased', 'excitement'],\n",
        "            'neutral': ['neutral', 'neu', 'normal', 'baseline'],\n",
        "            'sad': ['sad', 'sadness', 'sorrow', 'melancholy', 'depression'],\n",
        "            'surprised': ['surprise', 'surprised', 'sur', 'amazed', 'ps', 'pleasant_surprised', 'shock']\n",
        "        }\n",
        "\n",
        "    def extract_emotion_clean(self, filepath, dataset_name):\n",
        "        \"\"\"Extract emotion using proven methods only\"\"\"\n",
        "        self.extraction_stats['total'] += 1\n",
        "\n",
        "        filename = os.path.basename(filepath)\n",
        "        filename_lower = filename.lower()\n",
        "        full_path_lower = filepath.lower()\n",
        "\n",
        "        emotion = None\n",
        "\n",
        "        # Dataset-specific extraction\n",
        "        if 'ravdess' in dataset_name.lower():\n",
        "            emotion = self._extract_ravdess(filename)\n",
        "        elif 'crema' in dataset_name.lower():\n",
        "            emotion = self._extract_crema(filename, full_path_lower)\n",
        "        elif 'tess' in dataset_name.lower():\n",
        "            emotion = self._extract_tess(filename, full_path_lower)\n",
        "        elif 'emodb' in dataset_name.lower() or 'berlin' in dataset_name.lower():\n",
        "            emotion = self._extract_emodb(filename)\n",
        "        elif 'savee' in dataset_name.lower():\n",
        "            emotion = self._extract_savee(filename)\n",
        "        else:\n",
        "            emotion = self._extract_universal(filename, full_path_lower)\n",
        "\n",
        "        # Store debug info\n",
        "        if len(self.debug_extractions) < 30:\n",
        "            self.debug_extractions.append(f\"{dataset_name}: {filename} â†’ {emotion}\")\n",
        "\n",
        "        if emotion and emotion != 'unknown':\n",
        "            self.extraction_stats['success'] += 1\n",
        "            return emotion\n",
        "        else:\n",
        "            self.extraction_stats['failed'] += 1\n",
        "            return 'unknown'\n",
        "\n",
        "    def _extract_ravdess(self, filename):\n",
        "        \"\"\"RAVDESS extraction\"\"\"\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        parts = base_name.split('-')\n",
        "\n",
        "        if len(parts) >= 3:\n",
        "            emotion_code = parts[2]\n",
        "            ravdess_emotions = {\n",
        "                '01': 'neutral', '02': 'calm', '03': 'happy',\n",
        "                '04': 'sad', '05': 'angry', '06': 'fearful',\n",
        "                '07': 'disgust', '08': 'surprised'\n",
        "            }\n",
        "            return ravdess_emotions.get(emotion_code, 'unknown')\n",
        "\n",
        "        return 'unknown'\n",
        "\n",
        "    def _extract_crema(self, filename, full_path):\n",
        "        \"\"\"CREMA-D extraction\"\"\"\n",
        "        filename_upper = filename.upper()\n",
        "\n",
        "        # Handle mixed datasets\n",
        "        if any(x in full_path for x in ['tess', 'yaf_', 'oaf_']):\n",
        "            return self._extract_tess(filename, full_path)\n",
        "        if 'savee' in full_path:\n",
        "            return self._extract_savee(filename)\n",
        "        if re.match(r'\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}\\.wav', filename):\n",
        "            return self._extract_ravdess(filename)\n",
        "\n",
        "        # CREMA-D patterns\n",
        "        crema_emotions = {\n",
        "            'ANG': 'angry', 'HAP': 'happy', 'SAD': 'sad',\n",
        "            'NEU': 'neutral', 'FEA': 'fearful', 'DIS': 'disgust',\n",
        "            'SUR': 'surprised', 'CAL': 'calm'\n",
        "        }\n",
        "\n",
        "        for code, emotion in crema_emotions.items():\n",
        "            if f'_{code}_' in filename_upper or code in filename_upper:\n",
        "                return emotion\n",
        "\n",
        "        return 'unknown'\n",
        "\n",
        "    def _extract_tess(self, filename, full_path):\n",
        "        \"\"\"TESS extraction\"\"\"\n",
        "        filename_lower = filename.lower()\n",
        "\n",
        "        tess_patterns = {\n",
        "            '_angry': 'angry', '_anger': 'angry',\n",
        "            '_disgust': 'disgust', '_disgusted': 'disgust',\n",
        "            '_fear': 'fearful', '_fearful': 'fearful',\n",
        "            '_happy': 'happy', '_happiness': 'happy',\n",
        "            '_neutral': 'neutral',\n",
        "            '_sad': 'sad', '_sadness': 'sad',\n",
        "            '_ps': 'surprised',\n",
        "            '_surprise': 'surprised', '_surprised': 'surprised'\n",
        "        }\n",
        "\n",
        "        for pattern, emotion in tess_patterns.items():\n",
        "            if pattern in filename_lower:\n",
        "                return emotion\n",
        "\n",
        "        if '_' in filename_lower:\n",
        "            parts = filename_lower.split('_')\n",
        "            if len(parts) >= 3:\n",
        "                emotion_part = parts[2].replace('.wav', '').replace('.mp3', '')\n",
        "                for pattern, emotion in tess_patterns.items():\n",
        "                    if pattern.strip('_') == emotion_part:\n",
        "                        return emotion\n",
        "\n",
        "        return 'unknown'\n",
        "\n",
        "    def _extract_emodb(self, filename):\n",
        "        \"\"\"EMO-DB extraction\"\"\"\n",
        "        base_name = os.path.splitext(filename)[0].upper()\n",
        "\n",
        "        emodb_emotions = {\n",
        "            'W': 'angry', 'L': 'neutral', 'E': 'disgust',\n",
        "            'A': 'fearful', 'F': 'happy', 'T': 'sad', 'N': 'neutral'\n",
        "        }\n",
        "\n",
        "        positions_to_try = [-2, -3, 5, 6, 4, 3]\n",
        "\n",
        "        for pos in positions_to_try:\n",
        "            try:\n",
        "                if pos < 0 and len(base_name) >= abs(pos):\n",
        "                    emotion_char = base_name[pos]\n",
        "                    if emotion_char in emodb_emotions:\n",
        "                        return emodb_emotions[emotion_char]\n",
        "                elif pos >= 0 and len(base_name) > pos:\n",
        "                    emotion_char = base_name[pos]\n",
        "                    if emotion_char in emodb_emotions:\n",
        "                        return emodb_emotions[emotion_char]\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        return 'unknown'\n",
        "\n",
        "    def _extract_savee(self, filename):\n",
        "        \"\"\"SAVEE extraction\"\"\"\n",
        "        base_name = os.path.splitext(filename)[0].lower()\n",
        "\n",
        "        savee_patterns = {\n",
        "            'sa': 'sad', 'su': 'surprised',\n",
        "            'a': 'angry', 'h': 'happy', 'n': 'neutral',\n",
        "            'f': 'fearful', 'd': 'disgust'\n",
        "        }\n",
        "\n",
        "        for pattern in ['sa', 'su']:\n",
        "            if base_name.startswith(pattern) and len(base_name) > len(pattern):\n",
        "                if base_name[len(pattern):].replace('_', '').isdigit():\n",
        "                    return savee_patterns[pattern]\n",
        "\n",
        "        if len(base_name) >= 1:\n",
        "            first_char = base_name[0]\n",
        "            if first_char in ['a', 'h', 'n', 'f', 'd'] and len(base_name) > 1:\n",
        "                if base_name[1:].replace('_', '').isdigit() or base_name[1] == '_':\n",
        "                    return savee_patterns[first_char]\n",
        "\n",
        "        for speaker in ['dc', 'je', 'jk', 'kl']:\n",
        "            if speaker in base_name:\n",
        "                remaining = base_name.replace(speaker, '').strip('_')\n",
        "                if remaining:\n",
        "                    for pattern in ['sa', 'su']:\n",
        "                        if remaining.startswith(pattern):\n",
        "                            return savee_patterns[pattern]\n",
        "                    if len(remaining) >= 1 and remaining[0] in ['a', 'h', 'n', 'f', 'd']:\n",
        "                        return savee_patterns[remaining[0]]\n",
        "\n",
        "        return 'unknown'\n",
        "\n",
        "    def _extract_universal(self, filename, full_path):\n",
        "        \"\"\"Universal extraction\"\"\"\n",
        "        methods = [\n",
        "            self._extract_ravdess,\n",
        "            self._extract_emodb,\n",
        "            self._extract_savee,\n",
        "        ]\n",
        "\n",
        "        for method in methods:\n",
        "            try:\n",
        "                result = method(filename)\n",
        "                if result != 'unknown':\n",
        "                    return result\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        return self._extract_tess(filename, full_path)\n",
        "\n",
        "    def print_debug_extractions(self):\n",
        "        \"\"\"Print debug extractions\"\"\"\n",
        "        print(\"\\nğŸ” DEBUG: Clean emotion extractions:\")\n",
        "        for extraction in self.debug_extractions:\n",
        "            print(f\"   {extraction}\")\n",
        "\n",
        "        success_rate = self.extraction_stats['success'] / max(self.extraction_stats['total'], 1) * 100\n",
        "        print(f\"\\nğŸ“Š Clean extraction success rate: {success_rate:.1f}%\")\n",
        "\n",
        "# =============================================================================\n",
        "# 5. CLEAN DATA PREPARATION\n",
        "# =============================================================================\n",
        "\n",
        "class CleanDataPreparator:\n",
        "    \"\"\"Clean data preparation with real audio features only\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_extractor = CleanAudioFeatureExtractor()\n",
        "        self.emotion_extractor = CleanEmotionExtractor()\n",
        "\n",
        "    def prepare_clean_dataset(self, datasets, max_samples_per_emotion=1500):\n",
        "        \"\"\"Prepare dataset with clean audio features only\"\"\"\n",
        "        print(f\"\\nğŸ”§ CLEAN DATASET PREPARATION (AUDIO FEATURES ONLY)\")\n",
        "        print(f\"Target: {max_samples_per_emotion} samples per emotion\")\n",
        "        print(\"ğŸš« NO synthetic features - reliable deployment\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        all_features = []\n",
        "        all_emotions = []\n",
        "        all_sources = []\n",
        "\n",
        "        emotion_counts = {}\n",
        "        processing_stats = {}\n",
        "\n",
        "        datasets.sort(key=lambda x: x.get('priority', 999))\n",
        "\n",
        "        for dataset_info in datasets:\n",
        "            print(f\"\\nğŸ“Š Processing {dataset_info['name']}...\")\n",
        "            print(f\"   ğŸ¯ Expected Performance: {dataset_info.get('accuracy', 'N/A')}\")\n",
        "\n",
        "            processed_count = 0\n",
        "            failed_count = 0\n",
        "            unknown_count = 0\n",
        "            dataset_emotions = {}\n",
        "\n",
        "            files_to_process = dataset_info['files']\n",
        "            np.random.shuffle(files_to_process)\n",
        "\n",
        "            max_files_per_dataset = min(len(files_to_process), 6000)\n",
        "            files_to_process = files_to_process[:max_files_per_dataset]\n",
        "\n",
        "            for audio_file in tqdm(files_to_process, desc=f\"Processing {dataset_info['name']}\"):\n",
        "                try:\n",
        "                    emotion_needs_samples = any(emotion_counts.get(emotion, 0) < max_samples_per_emotion\n",
        "                                              for emotion in ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'])\n",
        "\n",
        "                    if not emotion_needs_samples:\n",
        "                        break\n",
        "\n",
        "                    emotion = self.emotion_extractor.extract_emotion_clean(\n",
        "                        audio_file, dataset_info['name']\n",
        "                    )\n",
        "\n",
        "                    if emotion == 'unknown':\n",
        "                        unknown_count += 1\n",
        "                        continue\n",
        "\n",
        "                    current_count = emotion_counts.get(emotion, 0)\n",
        "                    if current_count >= max_samples_per_emotion:\n",
        "                        continue\n",
        "\n",
        "                    # Extract CLEAN audio features only\n",
        "                    features = self.feature_extractor.extract_clean_audio_features(audio_file)\n",
        "\n",
        "                    if features and len(features) > 150:  # Ensure sufficient clean audio features\n",
        "                        all_features.append(list(features.values()))\n",
        "                        all_emotions.append(emotion)\n",
        "                        all_sources.append(dataset_info['name'])\n",
        "\n",
        "                        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1\n",
        "                        dataset_emotions[emotion] = dataset_emotions.get(emotion, 0) + 1\n",
        "                        processed_count += 1\n",
        "                    else:\n",
        "                        failed_count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    failed_count += 1\n",
        "\n",
        "            processing_stats[dataset_info['name']] = {\n",
        "                'processed': processed_count,\n",
        "                'failed': failed_count,\n",
        "                'unknown': unknown_count,\n",
        "                'emotions': dataset_emotions,\n",
        "                'total_files': len(files_to_process),\n",
        "                'success_rate': processed_count / len(files_to_process) if files_to_process else 0,\n",
        "                'accuracy': dataset_info.get('accuracy', 'N/A')\n",
        "            }\n",
        "\n",
        "            print(f\"   âœ… Processed: {processed_count} samples\")\n",
        "            print(f\"   âŒ Failed: {failed_count} samples\")\n",
        "            print(f\"   â“ Unknown emotions: {unknown_count} samples\")\n",
        "            print(f\"   ğŸ“Š Success rate: {processing_stats[dataset_info['name']]['success_rate']:.1%}\")\n",
        "            print(f\"   ğŸ“Š Emotions found: {dataset_emotions}\")\n",
        "\n",
        "        self.emotion_extractor.print_debug_extractions()\n",
        "\n",
        "        print(f\"\\nâœ… CLEAN DATASET PREPARATION COMPLETE!\")\n",
        "        print(f\"ğŸ“Š Total samples: {len(all_features)}\")\n",
        "        print(f\"ğŸ“Š Final emotion distribution: {emotion_counts}\")\n",
        "\n",
        "        if len(all_features) == 0:\n",
        "            print(\"âŒ No features extracted!\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        X = np.array(all_features, dtype=np.float32)\n",
        "        y = np.array(all_emotions)\n",
        "        sources = np.array(all_sources)\n",
        "\n",
        "        print(f\"ğŸ“Š CLEAN feature matrix shape: {X.shape}\")\n",
        "\n",
        "        return X, y, sources, emotion_counts\n",
        "\n",
        "# =============================================================================\n",
        "# 6. CLEAN MODEL TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "class CleanEmotionClassifier:\n",
        "    \"\"\"Clean emotion classifier with audio features only\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.best_model = None\n",
        "        self.scaler = RobustScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.feature_selector = None\n",
        "\n",
        "    def train_clean_models(self, X, y, test_size=0.2):\n",
        "        \"\"\"Train models with clean audio features only\"\"\"\n",
        "        print(\"\\nğŸ§  CLEAN MODEL TRAINING (AUDIO FEATURES ONLY)\")\n",
        "        print(\"ğŸš« NO synthetic features - reliable deployment\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "        class_names = self.label_encoder.classes_\n",
        "\n",
        "        print(f\"ğŸ“Š Classes: {list(class_names)}\")\n",
        "        print(f\"ğŸ“Š Class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_encoded, test_size=test_size, random_state=42, stratify=y_encoded\n",
        "        )\n",
        "\n",
        "        print(f\"ğŸ“Š Training set: {X_train.shape[0]} samples\")\n",
        "        print(f\"ğŸ“Š Test set: {X_test.shape[0]} samples\")\n",
        "        print(f\"ğŸ“Š CLEAN audio features: {X_train.shape[1]}\")\n",
        "\n",
        "        # Feature selection\n",
        "        print(\"\\nğŸ” CLEAN FEATURE SELECTION...\")\n",
        "        n_features = min(150, X_train.shape[1])  # Reasonable number for clean features\n",
        "        self.feature_selector = SelectKBest(score_func=f_classif, k=n_features)\n",
        "        X_train_selected = self.feature_selector.fit_transform(X_train, y_train)\n",
        "        X_test_selected = self.feature_selector.transform(X_test)\n",
        "        print(f\"ğŸ“Š Selected clean features: {X_train_selected.shape[1]}\")\n",
        "\n",
        "        # Scaling\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train_selected)\n",
        "        X_test_scaled = self.scaler.transform(X_test_selected)\n",
        "\n",
        "        # Class balancing\n",
        "        print(\"\\nâš–ï¸ CLEAN CLASS BALANCING...\")\n",
        "        try:\n",
        "            smote = BorderlineSMOTE(random_state=42, k_neighbors=3)\n",
        "            X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "            print(f\"ğŸ“Š Balanced training set: {X_train_balanced.shape[0]} samples\")\n",
        "        except:\n",
        "            try:\n",
        "                smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "                X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "                print(f\"ğŸ“Š Balanced training set: {X_train_balanced.shape[0]} samples\")\n",
        "            except:\n",
        "                print(\"   âš ï¸ SMOTE failed, using original data\")\n",
        "                X_train_balanced = X_train_scaled\n",
        "                y_train_balanced = y_train\n",
        "\n",
        "        # Clean model ensemble (proven algorithms only)\n",
        "        models = {\n",
        "            'Clean XGBoost': xgb.XGBClassifier(\n",
        "                n_estimators=600,\n",
        "                max_depth=10,\n",
        "                learning_rate=0.02,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                reg_alpha=0.1,\n",
        "                reg_lambda=0.1,\n",
        "                random_state=42,\n",
        "                eval_metric='mlogloss',\n",
        "                tree_method='hist'\n",
        "            ),\n",
        "            'Clean LightGBM': lgb.LGBMClassifier(\n",
        "                n_estimators=600,\n",
        "                max_depth=10,\n",
        "                learning_rate=0.02,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                reg_alpha=0.1,\n",
        "                reg_lambda=0.1,\n",
        "                random_state=42,\n",
        "                verbose=-1,\n",
        "                objective='multiclass',\n",
        "                metric='multi_logloss'\n",
        "            ),\n",
        "            'Clean Random Forest': RandomForestClassifier(\n",
        "                n_estimators=500,\n",
        "                max_depth=30,\n",
        "                min_samples_split=2,\n",
        "                min_samples_leaf=1,\n",
        "                max_features='sqrt',\n",
        "                random_state=42,\n",
        "                n_jobs=-1,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'Clean SVM': SVC(\n",
        "                C=10,\n",
        "                gamma='scale',\n",
        "                kernel='rbf',\n",
        "                probability=True,\n",
        "                random_state=42,\n",
        "                class_weight='balanced'\n",
        "            )\n",
        "        }\n",
        "\n",
        "        # Train and evaluate models\n",
        "        cv_results = {}\n",
        "        print(f\"\\nğŸ”„ CLEAN MODEL EVALUATION:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        for name, model in models.items():\n",
        "            print(f\"\\nğŸ§ª Training {name}...\")\n",
        "            try:\n",
        "                cv_scores = cross_val_score(\n",
        "                    model, X_train_balanced, y_train_balanced,\n",
        "                    cv=kfold, scoring='f1_macro', n_jobs=1\n",
        "                )\n",
        "\n",
        "                model.fit(X_train_balanced, y_train_balanced)\n",
        "                y_pred = model.predict(X_test_scaled)\n",
        "                test_accuracy = accuracy_score(y_test, y_pred)\n",
        "                test_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "                cv_results[name] = {\n",
        "                    'model': model,\n",
        "                    'cv_f1_mean': cv_scores.mean(),\n",
        "                    'cv_f1_std': cv_scores.std(),\n",
        "                    'test_accuracy': test_accuracy,\n",
        "                    'test_f1': test_f1\n",
        "                }\n",
        "\n",
        "                print(f\"   ğŸ“Š CV F1-Score: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}\")\n",
        "                print(f\"   ğŸ“Š Test Accuracy: {test_accuracy:.3f}\")\n",
        "                print(f\"   ğŸ“Š Test F1-Score: {test_f1:.3f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ Error with {name}: {e}\")\n",
        "\n",
        "        if not cv_results:\n",
        "            print(\"âŒ No models trained successfully!\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Create clean ensemble\n",
        "        print(f\"\\nğŸ¤ CREATING CLEAN ENSEMBLE...\")\n",
        "\n",
        "        sorted_models = sorted(cv_results.items(), key=lambda x: x[1]['test_f1'], reverse=True)\n",
        "        top_models = sorted_models[:3]\n",
        "\n",
        "        if len(top_models) >= 2:\n",
        "            ensemble_models = [(name, data['model']) for name, data in top_models]\n",
        "\n",
        "            ensemble = VotingClassifier(\n",
        "                estimators=ensemble_models,\n",
        "                voting='soft'\n",
        "            )\n",
        "\n",
        "            ensemble.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "            y_pred_ensemble = ensemble.predict(X_test_scaled)\n",
        "            ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
        "            ensemble_f1 = f1_score(y_test, y_pred_ensemble, average='macro')\n",
        "\n",
        "            print(f\"   ğŸ“Š Clean Ensemble Accuracy: {ensemble_accuracy:.3f}\")\n",
        "            print(f\"   ğŸ“Š Clean Ensemble F1-Score: {ensemble_f1:.3f}\")\n",
        "\n",
        "            cv_results['Clean Ensemble'] = {\n",
        "                'model': ensemble,\n",
        "                'test_accuracy': ensemble_accuracy,\n",
        "                'test_f1': ensemble_f1\n",
        "            }\n",
        "\n",
        "        # Select best model\n",
        "        best_model_name = max(cv_results.keys(), key=lambda x: cv_results[x]['test_f1'])\n",
        "        self.best_model = cv_results[best_model_name]['model']\n",
        "\n",
        "        print(f\"\\nğŸ† BEST CLEAN MODEL: {best_model_name}\")\n",
        "        print(f\"   ğŸ“Š Test Accuracy: {cv_results[best_model_name]['test_accuracy']:.3f}\")\n",
        "        print(f\"   ğŸ“Š Test F1-Score: {cv_results[best_model_name]['test_f1']:.3f}\")\n",
        "\n",
        "        # Final evaluation\n",
        "        y_pred_final = self.best_model.predict(X_test_scaled)\n",
        "\n",
        "        print(f\"\\nğŸ“Š DETAILED CLEAN PERFORMANCE ANALYSIS:\")\n",
        "        print(\"-\" * 60)\n",
        "        print(classification_report(y_test, y_pred_final, target_names=class_names, digits=3))\n",
        "\n",
        "        return cv_results, X_test_scaled, y_test, class_names\n",
        "\n",
        "# =============================================================================\n",
        "# 7. CLEAN MODEL SAVING\n",
        "# =============================================================================\n",
        "\n",
        "def save_clean_models(classifier, analytics, feature_extractor=None, save_dir=\"clean_audio_models\"):\n",
        "    \"\"\"Save clean audio-only models for deployment\"\"\"\n",
        "    import os\n",
        "    import joblib\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "\n",
        "    print(f\"\\nğŸ’¾ SAVING CLEAN AUDIO-ONLY MODELS\")\n",
        "    print(f\"ğŸ“‚ Target directory: {save_dir}\")\n",
        "    print(\"ğŸš« NO synthetic features - deployment ready\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Save clean models\n",
        "        model_path = os.path.join(save_dir, 'sota_xgboost_model.pkl')\n",
        "        joblib.dump(classifier.best_model, model_path)\n",
        "        model_size = os.path.getsize(model_path) / (1024*1024)\n",
        "        print(f\"âœ… Saved Clean Model: {model_path} ({model_size:.1f} MB)\")\n",
        "\n",
        "        scaler_path = os.path.join(save_dir, 'robust_scalar.pkl')\n",
        "        joblib.dump(classifier.scaler, scaler_path)\n",
        "        scaler_size = os.path.getsize(scaler_path) / 1024\n",
        "        print(f\"âœ… Saved Clean Scaler: {scaler_path} ({scaler_size:.1f} KB)\")\n",
        "\n",
        "        selector_path = os.path.join(save_dir, 'feature_selector.pkl')\n",
        "        joblib.dump(classifier.feature_selector, selector_path)\n",
        "        selector_size = os.path.getsize(selector_path) / 1024\n",
        "        print(f\"âœ… Saved Clean Feature Selector: {selector_path} ({selector_size:.1f} KB)\")\n",
        "\n",
        "        encoder_path = os.path.join(save_dir, 'label_encoder.pkl')\n",
        "        joblib.dump(classifier.label_encoder, encoder_path)\n",
        "        encoder_size = os.path.getsize(encoder_path) / 1024\n",
        "        print(f\"âœ… Saved Clean Label Encoder: {encoder_path} ({encoder_size:.1f} KB)\")\n",
        "\n",
        "        # Generate clean feature names\n",
        "        if feature_extractor and hasattr(feature_extractor, 'feature_names') and feature_extractor.feature_names:\n",
        "            feature_names = feature_extractor.feature_names\n",
        "        else:\n",
        "            feature_names = generate_clean_feature_names()\n",
        "\n",
        "        feature_names_path = os.path.join(save_dir, 'feature_names.pkl')\n",
        "        joblib.dump(feature_names, feature_names_path)\n",
        "        names_size = os.path.getsize(feature_names_path) / 1024\n",
        "        print(f\"âœ… Saved Clean Feature Names: {feature_names_path} ({len(feature_names)} features, {names_size:.1f} KB)\")\n",
        "\n",
        "        # Save clean metadata\n",
        "        metadata = {\n",
        "            'model_type': f\"Clean Audio-Only {analytics.get('best_model', 'Best Model')}\",\n",
        "            'author': 'Peter Chika Ozo-ogueji (Data Scientist)',\n",
        "            'accuracy': float(analytics['test_accuracy']),\n",
        "            'f1_score': float(analytics['test_f1_score']),\n",
        "            'feature_count': int(analytics['feature_count']),\n",
        "            'emotion_classes': list(classifier.label_encoder.classes_),\n",
        "            'dataset_sources': analytics.get('dataset_sources', []),\n",
        "            'sota_techniques': [\n",
        "                'Clean Audio Features Only',\n",
        "                'MFCC + Spectral + Prosodic',\n",
        "                'No Synthetic Features',\n",
        "                'Deployment Reliable'\n",
        "            ],\n",
        "            'training_date': datetime.now().isoformat(),\n",
        "            'extraction_success_rate': float(analytics.get('extraction_success_rate', 0)),\n",
        "            'total_samples': int(analytics['total_samples']),\n",
        "            'best_model_name': analytics.get('best_model', 'Clean Audio Model'),\n",
        "            'cv_score': float(analytics.get('cv_score', 0)),\n",
        "            'feature_selection_k': 150,\n",
        "            'scaling_method': 'RobustScaler',\n",
        "            'cross_validation': 'StratifiedKFold',\n",
        "            'deployment_ready': True,\n",
        "            'streamlit_compatible': True,\n",
        "            'synthetic_features_removed': True\n",
        "        }\n",
        "\n",
        "        metadata_path = os.path.join(save_dir, 'model_metadata.json')\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        metadata_size = os.path.getsize(metadata_path) / 1024\n",
        "        print(f\"âœ… Saved Clean Metadata: {metadata_path} ({metadata_size:.1f} KB)\")\n",
        "\n",
        "        total_size = sum([\n",
        "            os.path.getsize(model_path),\n",
        "            os.path.getsize(scaler_path),\n",
        "            os.path.getsize(selector_path),\n",
        "            os.path.getsize(encoder_path),\n",
        "            os.path.getsize(feature_names_path),\n",
        "            os.path.getsize(metadata_path)\n",
        "        ]) / (1024*1024)\n",
        "\n",
        "        print(f\"\\nğŸ‰ ALL CLEAN MODELS SAVED SUCCESSFULLY!\")\n",
        "        print(f\"ğŸ“‚ Location: {os.path.abspath(save_dir)}\")\n",
        "        print(f\"ğŸ“Š Total size: {total_size:.1f} MB\")\n",
        "        print(f\"ğŸ¯ Model accuracy: {analytics['test_accuracy']:.3f}\")\n",
        "        print(f\"ğŸ“ˆ F1-score: {analytics['test_f1_score']:.3f}\")\n",
        "        print(f\"ğŸ”¬ Clean features: {analytics['feature_count']}\")\n",
        "        print(f\"ğŸ“š Samples: {analytics['total_samples']:,}\")\n",
        "        print(f\"ğŸš« Synthetic features: REMOVED\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error saving clean models: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def generate_clean_feature_names():\n",
        "    \"\"\"Generate clean feature names (audio only)\"\"\"\n",
        "    feature_names = []\n",
        "\n",
        "    # MFCC features (13 Ã— 8 = 104 features)\n",
        "    for i in range(13):\n",
        "        for stat in ['mean', 'std', 'max', 'min', 'skew', 'kurtosis']:\n",
        "            feature_names.append(f'mfcc_{i}_{stat}')\n",
        "        feature_names.append(f'mfcc_delta_{i}_mean')\n",
        "        feature_names.append(f'mfcc_delta2_{i}_mean')\n",
        "\n",
        "    # Spectral features (4 Ã— 4 = 16 features)\n",
        "    for feature in ['spectral_centroid', 'spectral_rolloff', 'spectral_bandwidth', 'zero_crossing_rate']:\n",
        "        for stat in ['mean', 'std', 'max', 'skew']:\n",
        "            feature_names.append(f'{feature}_{stat}')\n",
        "\n",
        "    # Chroma features (12 Ã— 2 = 24 features)\n",
        "    for i in range(12):\n",
        "        for stat in ['mean', 'std']:\n",
        "            feature_names.append(f'chroma_{i}_{stat}')\n",
        "\n",
        "    # Prosodic features (11 features)\n",
        "    prosodic_features = ['f0_mean', 'f0_std', 'f0_range', 'f0_jitter', 'f0_shimmer',\n",
        "                        'f0_slope', 'f0_curvature', 'energy_mean', 'energy_std', 'energy_skew', 'energy_kurtosis']\n",
        "    feature_names.extend(prosodic_features)\n",
        "\n",
        "    # Advanced spectral features (16 features)\n",
        "    for i in range(7):\n",
        "        feature_names.append(f'spectral_contrast_{i}_mean')\n",
        "        feature_names.append(f'spectral_contrast_{i}_std')\n",
        "    feature_names.extend(['spectral_flatness_mean', 'spectral_flatness_std'])\n",
        "\n",
        "    # Harmonic features (15 features)\n",
        "    for i in range(6):\n",
        "        feature_names.append(f'tonnetz_{i}_mean')\n",
        "        feature_names.append(f'tonnetz_{i}_std')\n",
        "    feature_names.extend(['harmonic_energy', 'percussive_energy', 'harmonic_percussive_ratio'])\n",
        "\n",
        "    # Temporal features (5 features)\n",
        "    temporal_features = ['tempo', 'beat_count', 'beat_variance', 'onset_count', 'onset_rate']\n",
        "    feature_names.extend(temporal_features)\n",
        "\n",
        "    return feature_names\n",
        "\n",
        "# =============================================================================\n",
        "# 8. MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "def build_clean_production_system():\n",
        "    \"\"\"Build clean production system with audio features only\"\"\"\n",
        "    print(\"ğŸ—ï¸ BUILDING CLEAN PRODUCTION SYSTEM\")\n",
        "    print(\"ğŸš« NO synthetic features - reliable deployment\")\n",
        "    print(\"ğŸ‘¨â€ğŸ’» Author: Peter Chika Ozo-ogueji (Data Scientist)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    setup_kaggle()\n",
        "\n",
        "    datasets = download_sota_datasets()\n",
        "    if not datasets:\n",
        "        print(\"âŒ No datasets downloaded\")\n",
        "        return None, None\n",
        "\n",
        "    preparator = CleanDataPreparator()\n",
        "    result = preparator.prepare_clean_dataset(datasets, max_samples_per_emotion=1500)\n",
        "\n",
        "    if result[0] is None:\n",
        "        print(\"âŒ Data preparation failed\")\n",
        "        return None, None\n",
        "\n",
        "    X, y, sources, emotion_counts = result\n",
        "\n",
        "    if len(X) < 1500:\n",
        "        print(\"âŒ Insufficient data for training\")\n",
        "        return None, None\n",
        "\n",
        "    classifier = CleanEmotionClassifier()\n",
        "    cv_results, X_test, y_test, class_names = classifier.train_clean_models(X, y)\n",
        "\n",
        "    if cv_results is None:\n",
        "        print(\"âŒ Model training failed\")\n",
        "        return None, None\n",
        "\n",
        "    best_model_name = max(cv_results.keys(), key=lambda x: cv_results[x]['test_f1'])\n",
        "\n",
        "    analytics = {\n",
        "        'total_samples': len(X),\n",
        "        'emotion_distribution': emotion_counts,\n",
        "        'best_model': best_model_name,\n",
        "        'test_accuracy': cv_results[best_model_name]['test_accuracy'],\n",
        "        'test_f1_score': cv_results[best_model_name]['test_f1'],\n",
        "        'feature_count': X.shape[1],\n",
        "        'dataset_sources': list(set(sources)),\n",
        "        'extraction_success_rate': preparator.emotion_extractor.extraction_stats['success'] / max(preparator.emotion_extractor.extraction_stats['total'], 1) * 100,\n",
        "    }\n",
        "\n",
        "    accuracy_pct = analytics['test_accuracy'] * 100\n",
        "    f1_pct = analytics['test_f1_score'] * 100\n",
        "\n",
        "    print(f\"\\nğŸ‰ CLEAN PRODUCTION SYSTEM COMPLETE!\")\n",
        "    print(f\"ğŸ“Š Total samples: {analytics['total_samples']:,}\")\n",
        "    print(f\"ğŸ“Š Best clean model: {analytics['best_model']}\")\n",
        "    print(f\"ğŸ“Š Test accuracy: {accuracy_pct:.1f}%\")\n",
        "    print(f\"ğŸ“Š Test F1-score: {f1_pct:.1f}%\")\n",
        "    print(f\"ğŸ“Š Clean audio features: {analytics['feature_count']}\")\n",
        "    print(f\"ğŸ“Š Extraction success: {analytics['extraction_success_rate']:.1f}%\")\n",
        "    print(f\"ğŸš« Synthetic features: REMOVED\")\n",
        "\n",
        "    if accuracy_pct >= 80:\n",
        "        print(\"ğŸ¯ âœ… EXCELLENT PERFORMANCE: 80%+ ACCURACY!\")\n",
        "    elif accuracy_pct >= 75:\n",
        "        print(\"ğŸ¯ ğŸ”¶ GOOD PERFORMANCE: 75%+ ACCURACY!\")\n",
        "    else:\n",
        "        print(f\"ğŸ¯ ğŸ“ˆ BASELINE PERFORMANCE: {accuracy_pct:.1f}%\")\n",
        "\n",
        "    return classifier, analytics\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"ğŸš€ STARTING CLEAN AUDIO-ONLY SYSTEM...\")\n",
        "    print(\"ğŸš« NO synthetic features - reliable deployment\")\n",
        "    print(\"ğŸ‘¨â€ğŸ’» Author: Peter Chika Ozo-ogueji (Data Scientist)\")\n",
        "\n",
        "    try:\n",
        "        clean_classifier, clean_analytics = build_clean_production_system()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸ‰ CLEAN AUDIO-ONLY SPEECH ANALYTICS COMPLETE!\")\n",
        "        print(\"ğŸš« NO SYNTHETIC FEATURES - DEPLOYMENT READY!\")\n",
        "        print(\"ğŸ¯ RELIABLE PERFORMANCE WITH REAL AUDIO FEATURES!\")\n",
        "        print(\"ğŸ‘¨â€ğŸ’» Author: Peter Chika Ozo-ogueji (Data Scientist)\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        if clean_classifier and clean_analytics:\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"ğŸ’¾ SAVING CLEAN MODELS FOR DEPLOYMENT\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            try:\n",
        "                preparator = CleanDataPreparator()\n",
        "                feature_extractor = preparator.feature_extractor\n",
        "            except:\n",
        "                feature_extractor = None\n",
        "\n",
        "            save_success = save_clean_models(\n",
        "                classifier=clean_classifier,\n",
        "                analytics=clean_analytics,\n",
        "                feature_extractor=feature_extractor,\n",
        "                save_dir=\"clean_audio_models\"\n",
        "            )\n",
        "\n",
        "            if save_success:\n",
        "                print(\"\\nğŸ‰ CLEAN MODELS SAVED SUCCESSFULLY!\")\n",
        "                print(\"ğŸš« NO synthetic features - deployment ready!\")\n",
        "                print(\"ğŸš€ Your clean audio model should work reliably!\")\n",
        "            else:\n",
        "                print(\"\\nâŒ Failed to save clean models\")\n",
        "\n",
        "        return clean_classifier, clean_analytics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ System error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5flYbMh2TMwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}